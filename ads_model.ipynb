{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn import ensemble, metrics, model_selection, tree\n",
    "\n",
    "from ads_helpers import encode_labels, txt_to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract features from the datasets\n",
    "## combine datasets\n",
    "### 0 = no attack\n",
    "### 1 = attack detected\n",
    "\n",
    "data_dir = 'test_datasets'\n",
    "dataset_filenames = {'attack_free': 'Attack_free_dataset.txt',\n",
    "                     'dos_attack': 'DoS_attack_dataset.txt',\n",
    "                     'fuzzy_attack': 'Fuzzy_attack_dataset.txt',\n",
    "                     'impersonation_attack': 'Impersonation_attack_dataset.txt'}\n",
    "\n",
    "data_dfs = {}\n",
    "for dset in dataset_filenames:\n",
    "    attack_label = 0 if dset == 'attack_free' else 1\n",
    "    dset_df = txt_to_df(dataset_filenames, data_dir, dset)\n",
    "    dset_df['Attack'] = attack_label\n",
    "    data_dfs[f'{dset}_df'] = dset_df\n",
    "attack_df = pd.concat(data_dfs.values(), keys=data_dfs.keys())\n",
    "\n",
    "## preprocess non-numerical features\n",
    "attack_df = encode_labels(attack_df, label_lst=['ID', 'Data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ads_model.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## build ensemble model\n",
    "## modify and train model\n",
    "\n",
    "target_key = 'Attack'\n",
    "training_keys = [s for s in list(attack_df.keys()) if s != target_key]\n",
    "\n",
    "x = attack_df[training_keys]\n",
    "y = attack_df[target_key]\n",
    "\n",
    "num_trees = 30\n",
    "random_state = 42\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y, test_size=0.3, random_state=random_state)\n",
    "\n",
    "# initialize models\n",
    "dt_model = tree.DecisionTreeClassifier(random_state=random_state)\n",
    "rf_model = ensemble.RandomForestClassifier(n_estimators=num_trees, random_state=random_state)\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=num_trees, random_state=random_state)\n",
    "\n",
    "# combine predictions from multiple models with ensemble method --> majority voting scheme\n",
    "# each model \"votes\" for its predicted class and the class with the most votes becomes the final prediction\n",
    "voting_classifier = ensemble.VotingClassifier(estimators=[('decision_tree', dt_model),\n",
    "                                                          ('random_forest', rf_model),\n",
    "                                                          ('xg_boost', xgb_model)],\n",
    "                                              voting='hard') # hard for majority voting\n",
    "\n",
    "voting_classifier.fit(x_train, y_train)\n",
    "\n",
    "## export model\n",
    "joblib.dump(voting_classifier, 'ads_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating predictions for No Attack...\n",
      "Model Accuracy: 0.9490279126494185\n",
      "Model Detection Rate: 0.9454654080868119\n",
      "Model False Alarm Rate: 0.047130969076870866\n",
      "Model F1 Score: 0.9506092715695438\n",
      "\n",
      "Evaluating predictions for Attack Detected...\n",
      "Model Accuracy: 0.9490279126494185\n",
      "Model Detection Rate: 0.9528690309231291\n",
      "Model False Alarm Rate: 0.054534591913188074\n",
      "Model F1 Score: 0.9473419426013963\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## evaluate model\n",
    "\n",
    "pred = voting_classifier.predict(x_test)\n",
    "conf_matrix = metrics.confusion_matrix(y_test, pred)\n",
    "\n",
    "tp = conf_matrix.diagonal() # true positives\n",
    "tn = conf_matrix.sum() - conf_matrix.sum(axis=0) - conf_matrix.sum(axis=1) + tp # true negatives\n",
    "fp = conf_matrix.sum(axis=1) - tp # false positives\n",
    "fn = conf_matrix.sum(axis=0) - tp # false negatives\n",
    "\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "detection_rate = tp / (tp + fn)\n",
    "false_alarm_rate = fp / (tn + fp)\n",
    "f1 = 2*tp / (2*tp + fp + fn)\n",
    "\n",
    "for i, val in enumerate(set(y_test)):\n",
    "    if val == 0:\n",
    "        attack_type = 'No Attack'\n",
    "    elif val == 1:\n",
    "        attack_type = 'Attack Detected'\n",
    "\n",
    "    print(f\"Evaluating predictions for {attack_type}...\")\n",
    "    print(f\"Model Accuracy: {accuracy[i]}\")\n",
    "    print(f\"Model Detection Rate: {detection_rate[i]}\")\n",
    "    print(f\"Model False Alarm Rate: {false_alarm_rate[i]}\")\n",
    "    print(f\"Model F1 Score: {f1[i]}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
